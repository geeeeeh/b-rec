import streamlit as st
import json, re, datetime
from collections import Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# =========================
# ê¸°ë³¸ ì„¸íŒ…
# =========================
st.set_page_config(page_title="êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ê¸°ë°˜ ë„ì„œ ì¶”ì²œ ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ“š êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ê¸°ë°˜ ë„ì„œ ì¶”ì²œ ì‹œìŠ¤í…œ")
st.caption("JSON ì—…ë¡œë“œ â†’ í˜ì´ì§€/í‚¤ì›Œë“œ í•„í„° â†’ ì±… ê²€ìƒ‰í˜• / í‚¤ì›Œë“œí˜• ì¶”ì²œ Â· ì£¼ì œ/ì„¤ëª…/ì €ì/ì¶œíŒì‚¬ ê°€ì¤‘ì¹˜ + ì¶œê°„ì¼ ìµœê·¼ 5ë…„ ê°€ì¤‘ì¹˜")

# =========================
# ì•ˆì „ ë¡œë” / ìœ í‹¸
# =========================
def safe_load_json(file):
    """UTF-8 BOM ì œê±° + ì—¬ëŸ¬ JSONì´ ë¶™ì€ ê²½ìš° ì²« ê°ì²´ë§Œ íŒŒì‹±"""
    text = file.read().decode("utf-8-sig")
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        try:
            decoder = json.JSONDecoder()
            obj, _ = decoder.raw_decode(text.lstrip())
            return obj
        except Exception as e:
            st.error(f"âŒ JSON íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: {e}")
            return None

def to_text(v):
    """ê°’ì„ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if v is None:
        return ""
    if isinstance(v, str):
        return v
    if isinstance(v, (int, float, bool)):
        return str(v)
    if isinstance(v, list):
        return " ".join(to_text(x) for x in v)
    if isinstance(v, dict):
        return " ".join(to_text(x) for x in v.values())
    return str(v)

def to_list(v):
    """subject ê°™ì€ í•„ë“œë¥¼ ë¦¬ìŠ¤íŠ¸[str]ë¡œ ë³€í™˜"""
    if v is None:
        return []
    if isinstance(v, list):
        out = []
        for x in v:
            if isinstance(x, str):
                s = x.strip()
                if s: out.append(s)
            elif isinstance(x, dict):
                for val in x.values():
                    s = to_text(val).strip()
                    if s: out.append(s)
            else:
                s = to_text(x).strip()
                if s: out.append(s)
        return out
    if isinstance(v, dict):
        return [to_text(x).strip() for x in v.values() if to_text(x).strip()]
    if isinstance(v, str):
        return [v.strip()] if v.strip() else []
    s = to_text(v).strip()
    return [s] if s else []

YEAR_RE = re.compile(r"(19|20)\d{2}")
PAGES_RE = re.compile(r"(\d+)\s*p\b", re.IGNORECASE)

def extract_year(book):
    """issuedYear/issued/datePublished ë“±ì—ì„œ 4ìë¦¬ ì—°ë„ ì¶”ì¶œ"""
    for c in [to_text(book.get("issuedYear")),
              to_text(book.get("issued")),
              to_text(book.get("datePublished")),
              to_text(book.get("publicationDate"))]:
        m = YEAR_RE.search(c)
        if m:
            try: return int(m.group(0))
            except: pass
    return None

def extract_pages(book):
    """extentì—ì„œ 'ìˆ«ì+p' ì¶”ì¶œ â†’ ìµœëŒ€ê°’ì„ í˜ì´ì§€ ìˆ˜ë¡œ ì‚¬ìš©"""
    ext = to_list(book.get("extent"))
    found = []
    for token in ext:
        for m in PAGES_RE.finditer(token):
            try: found.append(int(m.group(1)))
            except: pass
    return max(found) if found else None

def recency_weight(year, now_year=None):
    """ìµœê·¼ 5ë…„ ì„ í˜• ê°€ì¤‘ì¹˜ (ì˜¬í•´=1.0 â€¦ 5ë…„ ì´ìƒ=0)"""
    if year is None: return 0.0
    if now_year is None: now_year = datetime.date.today().year
    d = now_year - year
    d = max(d, 0)
    if d <= 5: return (5 - d) / 5.0
    return 0.0

# ì¶”ì²œ ê²°ê³¼ì— ë¶™ì¼ ê´€ë ¨ í‚¤ì›Œë“œ 2~3ê°œ ì„ ì •
def pick_related_keywords(subjects, picked_keywords=None, top_n=3):
    subs = [s for s in subjects if s]
    if not subs:
        return []
    picked_set = set([k.strip() for k in (picked_keywords or []) if k.strip()])
    # 1ìˆœìœ„: ì‚¬ìš©ìê°€ ì„ íƒí•œ í‚¤ì›Œë“œì™€ì˜ êµì§‘í•©
    inter = [s for s in subs if s in picked_set]
    result = inter[:top_n]
    # ë¶€ì¡±í•˜ë©´ subjectì—ì„œ ì•ìª½ í‚¤ì›Œë“œë¡œ ì±„ìš°ê¸°
    if len(result) < top_n:
        for s in subs:
            if s not in result:
                result.append(s)
            if len(result) >= top_n:
                break
    return result[:top_n]

# =========================
# ë°ì´í„° ë³€í™˜
# =========================
def build_records(data):
    """
    ê° ë„ì„œ ë ˆì½”ë“œ:
    {
      'title': str,
      'subjects': List[str],
      'desc': str,
      'creator': str,
      'publisher': str,
      'year': int|None,
      'pages': int|None,
      'raw': dict
    }
    """
    if not isinstance(data, dict): return []
    books = data.get("@graph", [])
    if not isinstance(books, list) or not books: return []
    recs = []
    for bk in books:
        recs.append({
            "title": to_text(bk.get("title")) or "(ì œëª© ì—†ìŒ)",
            "subjects": to_list(bk.get("subject")),
            "desc": to_text(bk.get("description")),
            "creator": to_text(bk.get("creator")),
            "publisher": to_text(bk.get("publisher")),
            "year": extract_year(bk),
            "pages": extract_pages(bk),
            "raw": bk,
        })
    return recs

# =========================
# ì—…ë¡œë“œ
# =========================
uploaded = st.file_uploader("ë„ì„œì •ë³´ JSON íŒŒì¼ ì—…ë¡œë“œ", type=["json"])
if not uploaded:
    st.info("ğŸ“‚ ë¨¼ì € JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    st.stop()

data = safe_load_json(uploaded)
if data is None:
    st.stop()

records = build_records(data)
if not records:
    st.warning("âš ï¸ '@graph' ë‚´ ë„ì„œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    st.stop()

# =========================
# í˜ì´ì§€ í•„í„°
# =========================
pages_list = [r["pages"] for r in records if r["pages"] is not None]
min_pages, max_pages = (int(min(pages_list)), int(max(pages_list))) if pages_list else (0, 2000)

st.sidebar.header("ğŸ” í•„í„° & ê°€ì¤‘ì¹˜")
include_no_pages = st.sidebar.checkbox("ìª½ìˆ˜ ì •ë³´ ì—†ëŠ” ìë£Œë„ í¬í•¨", value=True)
page_range = st.sidebar.slider("í˜ì´ì§€(ìª½) ë²”ìœ„", min_value=min_pages, max_value=max_pages,
                               value=(min_pages, max_pages))

def pass_page_filter(p):
    if p is None:
        return include_no_pages
    return page_range[0] <= p <= page_range[1]

filtered = [r for r in records if pass_page_filter(r["pages"])]
if not filtered:
    st.warning("âš ï¸ í˜ì´ì§€ í•„í„° ì¡°ê±´ì— ë§ëŠ” ë„ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë²”ìœ„ë¥¼ ë„“í˜€ì£¼ì„¸ìš”.")
    st.stop()

# =========================
# ìƒìœ„ 10 í‚¤ì›Œë“œ
# =========================
all_subjects = []
for r in filtered:
    all_subjects.extend(r["subjects"])
top_keywords = [kw for kw, _ in Counter([s for s in all_subjects if s]).most_common(10)]

# =========================
# í•„ë“œë³„ ë§ë­‰ì¹˜ (ì œëª©/KDC ì œì™¸)
# =========================
titles = [r["title"] for r in filtered]
subject_texts = [" ".join(r["subjects"]) for r in filtered]
desc_texts = [r["desc"] for r in filtered]
author_texts = [r["creator"] for r in filtered]
publisher_texts = [r["publisher"] for r in filtered]
years = [r["year"] for r in filtered]
pages = [r["pages"] for r in filtered]
raw_books = [r["raw"] for r in filtered]
subjects_by_idx = [r["subjects"] for r in filtered]

# ë²¡í„°ë¼ì´ì €(í•„ë“œë³„)
vec_subj = TfidfVectorizer()
vec_desc = TfidfVectorizer()
vec_auth = TfidfVectorizer()
vec_pub  = TfidfVectorizer()

X_subj  = vec_subj.fit_transform(subject_texts)
X_desc  = vec_desc.fit_transform(desc_texts)
X_auth  = vec_auth.fit_transform(author_texts)
X_pub   = vec_pub.fit_transform(publisher_texts)

now_year = datetime.date.today().year
recency_vec = np.array([recency_weight(y, now_year) for y in years], dtype=float)

# =========================
# ê°€ì¤‘ì¹˜ UI (ì œëª©/KDC ì œê±°)
# =========================
st.sidebar.markdown("### âš–ï¸ ì„¸ë¶€ ê°€ì¤‘ì¹˜ (ì½˜í…ì¸ )")
w_subj = st.sidebar.slider("ì£¼ì œ(í‚¤ì›Œë“œ) ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.45, 0.05)
w_desc = st.sidebar.slider("ì„¤ëª…(ìš”ì•½) ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.30, 0.05)
w_auth = st.sidebar.slider("ì €ì ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.15, 0.05)
w_pub  = st.sidebar.slider("ì¶œíŒì‚¬ ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.10, 0.05)

w_sum = w_subj + w_desc + w_auth + w_pub
if w_sum == 0:
    w_subj, w_desc, w_auth, w_pub = 0.45, 0.30, 0.15, 0.10
    w_sum = 1.0
w_subj, w_desc, w_auth, w_pub = [w / w_sum for w in [w_subj, w_desc, w_auth, w_pub]]

st.sidebar.markdown("### â± ì¶œê°„ì¼ ìµœê·¼ 5ë…„ ê°€ì¤‘ì¹˜")
w_recency = st.sidebar.slider("ì¶œê°„ì¼ ìµœê·¼ 5ë…„ ê°€ì¤‘ì¹˜", 0.0, 0.8, 0.30, 0.05,
                              help="ìµœì¢… ì ìˆ˜ = (1-ë¹„ìœ¨)*ì½˜í…ì¸ ì ìˆ˜ + (ë¹„ìœ¨)*ìµœê·¼ì„±")
top_n = st.sidebar.slider("ì¶”ì²œ ê°œìˆ˜ (Top N)", 3, 15, 5)

def combine_content_score(s_subj, s_desc, s_auth, s_pub):
    return (w_subj*s_subj + w_desc*s_desc + w_auth*s_auth + w_pub*s_pub)

def final_score(content_sim, rec_vec):
    return (1 - w_recency) * content_sim + w_recency * rec_vec

# =========================
# ë ˆì´ì•„ì›ƒ
# =========================
col1, col2 = st.columns(2, vertical_alignment="top")

# ---------- A) ì±… ì„ íƒí˜• (ê²€ìƒ‰ ê¸°ë°˜) ----------
with col1:
    st.subheader("ğŸ”– ì±… ê²€ìƒ‰í˜• ì¶”ì²œ ")
    # ê²€ìƒ‰ì–´ ì…ë ¥
    query_title = st.text_input("ì œëª© ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ë¶€ë¶„ì¼ì¹˜ ì§€ì›)", placeholder="ì˜ˆ: ë„ì„œê´€í•™, ì €ì‘ê¶Œ, ë””ì§€í„¸ë„ì„œê´€")
    # ê²€ìƒ‰ ì‹¤í–‰ ë²„íŠ¼
    if "matched_indices" not in st.session_state:
        st.session_state.matched_indices = []

    if st.button("ê²€ìƒ‰"):
        q = query_title.strip().lower()
        if not q:
            st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            st.session_state.matched_indices = []
        else:
            matches = [i for i, t in enumerate(titles) if q in t.lower()]
            if not matches:
                st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ë³´ì„¸ìš”.")
            st.session_state.matched_indices = matches

    # ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡ì—ì„œ ì„ íƒ
    if st.session_state.matched_indices:
        options = [titles[i] for i in st.session_state.matched_indices]
        sel_title = st.selectbox("ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê¸°ì¤€ ë„ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”", options=options, index=0, key="select_matched_title")

        if st.button("ì´ ì±…ê³¼ ë¹„ìŠ·í•œ ë„ì„œ ì¶”ì²œ", use_container_width=True):
            # ì„ íƒ ì œëª©ì˜ ì „ì—­ ì¸ë±ìŠ¤ ì°¾ê¸°
            target_title = st.session_state.select_matched_title
            # ì „ì—­ ì¸ë±ìŠ¤ (filtered ë‚´)
            idx = None
            for i in st.session_state.matched_indices:
                if titles[i] == target_title:
                    idx = i
                    break
            if idx is None:
                st.error("ì„ íƒí•œ ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            else:
                s_subj = cosine_similarity(X_subj[idx], X_subj).flatten()
                s_desc = cosine_similarity(X_desc[idx], X_desc).flatten()
                s_auth = cosine_similarity(X_auth[idx], X_auth).flatten()
                s_pub  = cosine_similarity(X_pub[idx],  X_pub ).flatten()

                content_sim = combine_content_score(s_subj, s_desc, s_auth, s_pub)
                final = final_score(content_sim, recency_vec)

                order = final.argsort()[::-1]
                recs = [i for i in order if i != idx][:top_n]

                st.write(f"**ê¸°ì¤€ ë„ì„œ:** {target_title}")
                if not recs:
                    st.info("ì¶”ì²œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    for i in recs:
                        creator = to_text(raw_books[i].get("creator")) or "ì €ì ì •ë³´ ì—†ìŒ"
                        y = years[i] or "N/A"
                        p = pages[i] if pages[i] is not None else "N/A"
                        rel_keywords = pick_related_keywords(subjects_by_idx[i], picked_keywords=None, top_n=3)
                        kw_disp = " Â· ê´€ë ¨ í‚¤ì›Œë“œ: " + ", ".join(rel_keywords) if rel_keywords else ""
                        st.markdown(
                            f"- **{titles[i]}** â€” {creator} (ì—°ë„: {y}, ìª½ìˆ˜: {p})  "
                            f"Â· ì½˜í…ì¸ ì ìˆ˜: {content_sim[i]:.3f} Â· ìµœì¢…ì ìˆ˜: {final[i]:.3f}{kw_disp}"
                        )
    else:
        st.caption("ê²€ìƒ‰ í›„ ê²°ê³¼ ëª©ë¡ì—ì„œ ê¸°ì¤€ ë„ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”.")

# ---------- B) í‚¤ì›Œë“œ ê²€ìƒ‰í˜• ----------
with col2:
    st.subheader("ğŸ“ í‚¤ì›Œë“œ ê²€ìƒ‰í˜• ì¶”ì²œ")
    st.caption("ìƒìœ„ 10 í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ê±°ë‚˜, ììœ  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    picked = st.multiselect("ìƒìœ„ í‚¤ì›Œë“œ", options=top_keywords, default=[])
    q = st.text_input("ììœ  í‚¤ì›Œë“œ (ì„ íƒ)", placeholder="ì˜ˆ: ë„ì„œê´€í•™, ì €ì‘ê¶Œë²•, ì—­ì‚¬")

    if st.button("í‚¤ì›Œë“œë¡œ ì¶”ì²œ", use_container_width=True):
        if not picked and not q.strip():
            st.warning("í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        else:
            query = " ".join(picked + ([q.strip()] if q.strip() else []))
            q_subj = vec_subj.transform([query])
            q_desc = vec_desc.transform([query])
            q_auth = vec_auth.transform([query])
            q_pub  = vec_pub.transform([query])

            s_subj = cosine_similarity(q_subj, X_subj).flatten()
            s_desc = cosine_similarity(q_desc, X_desc).flatten()
            s_auth = cosine_similarity(q_auth, X_auth).flatten()
            s_pub  = cosine_similarity(q_pub,  X_pub ).flatten()

            content_sim = combine_content_score(s_subj, s_desc, s_auth, s_pub)
            final = final_score(content_sim, recency_vec)

            order = final.argsort()[::-1][:top_n]
            st.write(f"**ì…ë ¥/ì„ íƒ í‚¤ì›Œë“œ:** {query}")
            for i in order:
                creator = to_text(raw_books[i].get("creator")) or "ì €ì ì •ë³´ ì—†ìŒ"
                y = years[i] or "N/A"
                p = pages[i] if pages[i] is not None else "N/A"
                rel_keywords = pick_related_keywords(subjects_by_idx[i], picked_keywords=picked, top_n=3)
                kw_disp = " Â· ê´€ë ¨ í‚¤ì›Œë“œ: " + ", ".join(rel_keywords) if rel_keywords else ""
                st.markdown(
                    f"- **{titles[i]}** â€” {creator} (ì—°ë„: {y}, ìª½ìˆ˜: {p})  "
                    f"Â· ì½˜í…ì¸ ì ìˆ˜: {content_sim[i]:.3f} Â· ìµœì¢…ì ìˆ˜: {final[i]:.3f}{kw_disp}"
                )
