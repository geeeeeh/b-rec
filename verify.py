import streamlit as st
import json, re, datetime
from collections import Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

st.set_page_config(page_title="ë„ì„œ ì¶”ì²œ ì‹œìŠ¤í…œ", layout="wide")
st.title("ğŸ“š êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ê¸°ë°˜ ë„ì„œ ì¶”ì²œ ì‹œìŠ¤í…œ")
st.caption("JSON ì—…ë¡œë“œ â†’ í˜ì´ì§€ìˆ˜/ì—°ë„/KDC/í‚¤ì›Œë“œ í•„í„° â†’ ì±… ì„ íƒí˜• ë˜ëŠ” í‚¤ì›Œë“œí˜• ì¶”ì²œ (ìµœê·¼ 5ë…„ ê°€ì¤‘ì¹˜ ë°˜ì˜)")

# ---------------------------
# 1) JSON ì•ˆì „ ë¡œë”
# ---------------------------
def safe_load_json(file):
    """UTF-8 BOM ì œê±° + ì—¬ëŸ¬ JSONì´ ë¶™ì€ ê²½ìš° ì²« ê°ì²´ë§Œ íŒŒì‹±"""
    text = file.read().decode("utf-8-sig")
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        try:
            decoder = json.JSONDecoder()
            obj, _ = decoder.raw_decode(text.lstrip())
            return obj
        except Exception as e:
            st.error(f"âŒ JSON íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: {e}")
            return None

# ---------------------------
# 2) ìœ í‹¸: ì•ˆì „ ë¬¸ìì—´/ë¦¬ìŠ¤íŠ¸ ë³€í™˜
# ---------------------------
def to_text(v):
    """ê°’ì„ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜: list/dict/None/ìˆ«ì ì „ë¶€ OK"""
    if v is None:
        return ""
    if isinstance(v, str):
        return v
    if isinstance(v, (int, float, bool)):
        return str(v)
    if isinstance(v, list):
        return " ".join(to_text(x) for x in v)
    if isinstance(v, dict):
        return " ".join(to_text(x) for x in v.values())
    return str(v)

def to_list(v):
    """subject ê°™ì€ í•„ë“œë¥¼ ë¦¬ìŠ¤íŠ¸[str]ë¡œ ë³€í™˜"""
    if v is None:
        return []
    if isinstance(v, list):
        out = []
        for x in v:
            if isinstance(x, str):
                out.append(x.strip())
            elif isinstance(x, dict):
                out.extend([to_text(val).strip() for val in x.values() if to_text(val).strip()])
            else:
                out.append(to_text(x).strip())
        return [s for s in out if s]
    if isinstance(v, dict):
        return [to_text(x).strip() for x in v.values() if to_text(x).strip()]
    if isinstance(v, str):
        return [v.strip()] if v.strip() else []
    return [to_text(v).strip()]

# ---------------------------
# 3) ë©”íƒ€ ì¶”ì¶œ: ì¶œíŒë…„ë„/í˜ì´ì§€ìˆ˜
# ---------------------------
YEAR_RE = re.compile(r"(19|20)\d{2}")
PAGES_RE = re.compile(r"(\d+)\s*p\b", re.IGNORECASE)  # ì˜ˆ: '586p'

def extract_year(book):
    """issuedYear/issued/datePublished ë“±ì—ì„œ 4ìë¦¬ ì—°ë„ ì¶”ì¶œ"""
    cand = [
        to_text(book.get("issuedYear")),
        to_text(book.get("issued")),
        to_text(book.get("datePublished")),
        to_text(book.get("publicationDate")),
    ]
    for c in cand:
        m = YEAR_RE.search(c)
        if m:
            try:
                return int(m.group(0))
            except:
                pass
    return None

def extract_pages(book):
    """
    extent ë°°ì—´ ì•ˆì—ì„œ 'ìˆ«ì+p' íŒ¨í„´ íƒì§€ â†’ ìµœëŒ€ê°’ì„ í˜ì´ì§€ ìˆ˜ë¡œ ì‚¬ìš©.
    ì˜ˆ: ['21cm','x, 586p'] â†’ 586
    """
    ext = to_list(book.get("extent"))
    pages_found = []
    for token in ext:
        for m in PAGES_RE.finditer(token):
            try:
                pages_found.append(int(m.group(1)))
            except:
                pass
    if pages_found:
        return max(pages_found)
    return None

def recency_weight(year, now_year=None):
    """
    ìµœê·¼ 5ë…„ ì„ í˜• ê°€ì¤‘ì¹˜.
    ì˜¬í•´=1.0, 1ë…„ ì „=0.8, â€¦, 5ë…„ ì´ìƒ ì§€ë‚œ ì±…=0
    """
    if year is None:
        return 0.0
    if now_year is None:
        now_year = datetime.date.today().year
    d = now_year - year
    if d < 0:
        d = 0
    if d <= 5:
        return (5 - d) / 5.0
    return 0.0

# ---------------------------
# 4) ì½”í¼ìŠ¤/íƒ€ì´í‹€/ë©”íƒ€ ìƒì„±
# ---------------------------
def build_records(data):
    """
    ê° ë„ì„œ ë ˆì½”ë“œë¥¼ ë‹¤ìŒ êµ¬ì¡°ë¡œ ë³€í™˜:
    {
      'title': str,
      'text': str (ì¶”ì²œìš© ë¬¸ì„œ),
      'year': int|None,
      'pages': int|None,
      'subjects': List[str],
      'raw': dict (ì›ë³¸)
    }
    """
    if not isinstance(data, dict):
        return []
    books = data.get("@graph", [])
    if not isinstance(books, list) or not books:
        return []

    records = []
    for bk in books:
        title = to_text(bk.get("title")) or "(ì œëª© ì—†ìŒ)"
        subjects = to_list(bk.get("subject"))
        parts = [
            title,
            to_text(bk.get("remainderOfTitle")),
            to_text(bk.get("creator")),
            to_text(bk.get("dcterms:creator")),
            " ".join(subjects),
            to_text(bk.get("description")),
            to_text(bk.get("titleOfSeries")),
            to_text(bk.get("publisher")),
            to_text(bk.get("kdc")),
            to_text(bk.get("publicationPlace")),
        ]
        text = " ".join(p for p in parts if p)
        rec = {
            "title": title,
            "text": text,
            "year": extract_year(bk),
            "pages": extract_pages(bk),
            "subjects": subjects,
            "raw": bk,
        }
        records.append(rec)
    return records

# ---------------------------
# ì—…ë¡œë“œ & ë³€í™˜
# ---------------------------
uploaded = st.file_uploader("ë„ì„œì •ë³´ JSON íŒŒì¼ ì—…ë¡œë“œ", type=["json"])
if not uploaded:
    st.info("ğŸ“‚ ë¨¼ì € JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    st.stop()

data = safe_load_json(uploaded)
if data is None:
    st.stop()

records = build_records(data)
if not records:
    st.warning("âš ï¸ '@graph' ë‚´ ë„ì„œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    st.stop()

# ---------------------------
# ì‚¬ì´ë“œë°” í•„í„°: í˜ì´ì§€ ìˆ˜ + ìµœê·¼ 5ë…„ ê°€ì¤‘ì¹˜
# ---------------------------
pages_list = [r["pages"] for r in records if r["pages"] is not None]
if pages_list:
    min_pages, max_pages = int(min(pages_list)), int(max(pages_list))
else:
    # í˜ì´ì§€ ì •ë³´ê°€ ì „í˜€ ì—†ì„ ë•Œ ì•ˆì „ ê¸°ë³¸ê°’
    min_pages, max_pages = 0, 2000

st.sidebar.header("ğŸ” í•„í„° & ê°€ì¤‘ì¹˜")
include_no_pages = st.sidebar.checkbox("ìª½ìˆ˜ ì •ë³´ ì—†ëŠ” ìë£Œë„ í¬í•¨", value=True)
page_range = st.sidebar.slider("í˜ì´ì§€(ìª½) ë²”ìœ„", min_value=min_pages, max_value=max_pages,
                               value=(min_pages, max_pages))

w_recency = st.sidebar.slider("ìµœê·¼ 5ë…„ ê°€ì¤‘ì¹˜ ë¹„ìœ¨", 0.0, 0.8, 0.3, 0.05,
                              help="ìµœì¢… ì ìˆ˜ = (1-ë¹„ìœ¨)*ë‚´ìš©ìœ ì‚¬ë„ + (ë¹„ìœ¨)*ìµœê·¼ì„±")
top_n = st.sidebar.slider("ì¶”ì²œ ê°œìˆ˜ (Top N)", 3, 15, 5)

# í˜ì´ì§€ í•„í„° ì ìš©
def pass_page_filter(p):
    if p is None:
        return include_no_pages
    return page_range[0] <= p <= page_range[1]

filtered = [r for r in records if pass_page_filter(r["pages"])]

if not filtered:
    st.warning("âš ï¸ í˜ì´ì§€ í•„í„° ì¡°ê±´ì— ë§ëŠ” ë„ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë²”ìœ„ë¥¼ ë„“í˜€ì£¼ì„¸ìš”.")
    st.stop()

# ---------------------------
# ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ(ì£¼ì œì–´) ì œì‹œ
# ---------------------------
all_subjects = []
for r in filtered:
    all_subjects.extend(r["subjects"])
freq = Counter([s for s in all_subjects if s])
top_keywords = [kw for kw, _ in freq.most_common(10)]

# ---------------------------
# TF-IDF (í•„í„°ëœ ë°ì´í„° ê¸°ì¤€)
# ---------------------------
titles = [r["title"] for r in filtered]
corpus = [r["text"] for r in filtered]
years = [r["year"] for r in filtered]
pages = [r["pages"] for r in filtered]
raw_books = [r["raw"] for r in filtered]

vectorizer = TfidfVectorizer()
try:
    tfidf = vectorizer.fit_transform(corpus)
except ValueError:
    st.error("í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ê°€ ë¹„ì–´ ìˆì–´ ë²¡í„°í™”ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

now_year = datetime.date.today().year
recency_vec = np.array([recency_weight(y, now_year) for y in years], dtype=float)

def rerank_with_recency(sim_scores):
    return (1 - w_recency) * sim_scores + (w_recency) * recency_vec

# ---------------------------
# ë ˆì´ì•„ì›ƒ: ì¢Œ(ì±… ì„ íƒí˜•) / ìš°(í‚¤ì›Œë“œí˜•)
# ---------------------------
col1, col2 = st.columns(2, vertical_alignment="top")

# ----- A) ì±… ì„ íƒí˜• -----
with col1:
    st.subheader("ğŸ”– ì±… ì„ íƒí˜• ì¶”ì²œ")
    sel_title = st.selectbox("ì¶”ì²œ ê¸°ì¤€ì´ ë  ì±…ì„ ì„ íƒí•˜ì„¸ìš”", options=titles, index=0)
    if st.button("ì´ ì±…ê³¼ ë¹„ìŠ·í•œ ë„ì„œ ì¶”ì²œ", use_container_width=True):
        idx = titles.index(sel_title)
        sims = cosine_similarity(tfidf[idx], tfidf).flatten()
        final = rerank_with_recency(sims)
        order = final.argsort()[::-1]
        recs = [i for i in order if i != idx][:top_n]

        st.write(f"**ê¸°ì¤€ ë„ì„œ:** {sel_title}")
        if not recs:
            st.info("ì¶”ì²œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            for i in recs:
                creator = to_text(raw_books[i].get("creator")) or "ì €ì ì •ë³´ ì—†ìŒ"
                kdc = to_text(raw_books[i].get("kdc")) or "N/A"
                y = years[i] or "N/A"
                p = pages[i] if pages[i] is not None else "N/A"
                base_sim = cosine_similarity(tfidf[idx], tfidf[i]).flatten()[0]
                st.markdown(
                    f"- **{titles[i]}** â€” {creator} "
                    f"(KDC: {kdc}, ì—°ë„: {y}, ìª½ìˆ˜: {p})  Â· ë‚´ìš©ìœ ì‚¬ë„: {base_sim:.3f}"
                )

# ----- B) í‚¤ì›Œë“œ ê²€ìƒ‰í˜• -----
with col2:
    st.subheader("ğŸ“ í‚¤ì›Œë“œ ê²€ìƒ‰í˜• ì¶”ì²œ")
    st.caption("ì•„ë˜ì—ì„œ ìƒìœ„ 10ê°œ í‚¤ì›Œë“œë¥¼ ê³ ë¥´ê±°ë‚˜, ì§ì ‘ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    picked = st.multiselect("ìƒìœ„ í‚¤ì›Œë“œ", options=top_keywords, default=[])
    q = st.text_input("ììœ  í‚¤ì›Œë“œ ì…ë ¥ (ì„ íƒ)", placeholder="ì˜ˆ: ë„ì„œê´€í•™, ì €ì‘ê¶Œë²•, ì—­ì‚¬")

    if st.button("í‚¤ì›Œë“œë¡œ ì¶”ì²œ", use_container_width=True):
        # ì„ íƒ í‚¤ì›Œë“œ + ììœ  í‚¤ì›Œë“œ ê²°í•©
        query_terms = []
        if picked:
            query_terms.append(" ".join(picked))
        if q.strip():
            query_terms.append(q.strip())
        if not query_terms:
            st.warning("í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        else:
            query = " ".join(query_terms)
            q_vec = vectorizer.transform([query])
            sims = cosine_similarity(q_vec, tfidf).flatten()
            final = rerank_with_recency(sims)
            order = final.argsort()[::-1][:top_n]

            st.write(f"**ì…ë ¥/ì„ íƒ í‚¤ì›Œë“œ:** {query}")
            for i in order:
                creator = to_text(raw_books[i].get("creator")) or "ì €ì ì •ë³´ ì—†ìŒ"
                kdc = to_text(raw_books[i].get("kdc")) or "N/A"
                y = years[i] or "N/A"
                p = pages[i] if pages[i] is not None else "N/A"
                base_sim = cosine_similarity(q_vec, tfidf[i]).flatten()[0]
                st.markdown(
                    f"- **{titles[i]}** â€” {creator} "
                    f"(KDC: {kdc}, ì—°ë„: {y}, ìª½ìˆ˜: {p})  Â· ë‚´ìš©ìœ ì‚¬ë„: {base_sim:.3f}"
                )
