import streamlit as st
import json, re, datetime
from collections import Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

st.set_page_config(page_title="ë„ì„œ ì¶”ì²œ ì‹œìŠ¤í…œ(ì„¸ë¶€ ê°€ì¤‘ì¹˜)", layout="wide")
st.title("ğŸ“š êµ­ë¦½ì¤‘ì•™ë„ì„œê´€ ê¸°ë°˜ ë„ì„œ ì¶”ì²œ ì‹œìŠ¤í…œ â€” ì„¸ë¶€ ê°€ì¤‘ì¹˜ ë²„ì „")
st.caption("JSON ì—…ë¡œë“œ â†’ í˜ì´ì§€/í‚¤ì›Œë“œ í•„í„° â†’ ì±… ì„ íƒí˜•/í‚¤ì›Œë“œí˜• ì¶”ì²œ Â· ì œëª©/ì£¼ì œ/ì„¤ëª…/ì €ì/ì¶œíŒì‚¬/KDC ê°œë³„ ê°€ì¤‘ì¹˜ + ìµœê·¼ì„± ê°€ì¤‘ì¹˜")

# ---------------------------
# ì•ˆì „ ë¡œë” / ìœ í‹¸
# ---------------------------
def safe_load_json(file):
    text = file.read().decode("utf-8-sig")
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        try:
            decoder = json.JSONDecoder()
            obj, _ = decoder.raw_decode(text.lstrip())
            return obj
        except Exception as e:
            st.error(f"âŒ JSON íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜: {e}")
            return None

def to_text(v):
    if v is None:
        return ""
    if isinstance(v, str):
        return v
    if isinstance(v, (int, float, bool)):
        return str(v)
    if isinstance(v, list):
        return " ".join(to_text(x) for x in v)
    if isinstance(v, dict):
        return " ".join(to_text(x) for x in v.values())
    return str(v)

def to_list(v):
    if v is None:
        return []
    if isinstance(v, list):
        out = []
        for x in v:
            if isinstance(x, str):
                if x.strip(): out.append(x.strip())
            elif isinstance(x, dict):
                for val in x.values():
                    s = to_text(val).strip()
                    if s: out.append(s)
            else:
                s = to_text(x).strip()
                if s: out.append(s)
        return out
    if isinstance(v, dict):
        return [to_text(x).strip() for x in v.values() if to_text(x).strip()]
    if isinstance(v, str):
        return [v.strip()] if v.strip() else []
    s = to_text(v).strip()
    return [s] if s else []

YEAR_RE = re.compile(r"(19|20)\d{2}")
PAGES_RE = re.compile(r"(\d+)\s*p\b", re.IGNORECASE)

def extract_year(book):
    for c in [to_text(book.get("issuedYear")),
              to_text(book.get("issued")),
              to_text(book.get("datePublished")),
              to_text(book.get("publicationDate"))]:
        m = YEAR_RE.search(c)
        if m:
            try: return int(m.group(0))
            except: pass
    return None

def extract_pages(book):
    ext = to_list(book.get("extent"))
    found = []
    for token in ext:
        for m in PAGES_RE.finditer(token):
            try: found.append(int(m.group(1)))
            except: pass
    return max(found) if found else None

def recency_weight(year, now_year=None):
    if year is None: return 0.0
    if now_year is None: now_year = datetime.date.today().year
    d = now_year - year
    d = max(d, 0)
    if d <= 5: return (5 - d) / 5.0
    return 0.0

# ---------------------------
# ë°ì´í„° ë³€í™˜
# ---------------------------
def build_records(data):
    if not isinstance(data, dict): return []
    books = data.get("@graph", [])
    if not isinstance(books, list) or not books: return []
    recs = []
    for bk in books:
        recs.append({
            "title": to_text(bk.get("title")) or "(ì œëª© ì—†ìŒ)",
            "subtitle": to_text(bk.get("remainderOfTitle")),
            "creator": to_text(bk.get("creator")),
            "subjects": to_list(bk.get("subject")),
            "desc": to_text(bk.get("description")),
            "series": to_text(bk.get("titleOfSeries")),
            "publisher": to_text(bk.get("publisher")),
            "kdc": to_text(bk.get("kdc")),
            "place": to_text(bk.get("publicationPlace")),
            "year": extract_year(bk),
            "pages": extract_pages(bk),
            "raw": bk
        })
    return recs

# ---------------------------
# ì—…ë¡œë“œ
# ---------------------------
uploaded = st.file_uploader("ë„ì„œì •ë³´ JSON íŒŒì¼ ì—…ë¡œë“œ", type=["json"])
if not uploaded:
    st.info("ğŸ“‚ ë¨¼ì € JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    st.stop()

data = safe_load_json(uploaded)
if data is None: st.stop()
records = build_records(data)
if not records:
    st.warning("âš ï¸ '@graph' ë‚´ ë„ì„œ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    st.stop()

# ---------------------------
# í˜ì´ì§€ í•„í„°
# ---------------------------
pages_list = [r["pages"] for r in records if r["pages"] is not None]
min_pages, max_pages = (int(min(pages_list)), int(max(pages_list))) if pages_list else (0, 2000)

st.sidebar.header("ğŸ” í•„í„° & ê°€ì¤‘ì¹˜")
include_no_pages = st.sidebar.checkbox("ìª½ìˆ˜ ì •ë³´ ì—†ëŠ” ìë£Œë„ í¬í•¨", value=True)
page_range = st.sidebar.slider("í˜ì´ì§€(ìª½) ë²”ìœ„", min_value=min_pages, max_value=max_pages,
                               value=(min_pages, max_pages))

def pass_page_filter(p):
    if p is None: return include_no_pages
    return page_range[0] <= p <= page_range[1]

filtered = [r for r in records if pass_page_filter(r["pages"])]
if not filtered:
    st.warning("âš ï¸ í˜ì´ì§€ í•„í„° ì¡°ê±´ì— ë§ëŠ” ë„ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë²”ìœ„ë¥¼ ë„“í˜€ì£¼ì„¸ìš”.")
    st.stop()

# ---------------------------
# ìƒìœ„ 10 í‚¤ì›Œë“œ ì œì‹œ
# ---------------------------
all_subjects = []
for r in filtered: all_subjects.extend(r["subjects"])
top_keywords = [kw for kw, _ in Counter([s for s in all_subjects if s]).most_common(10)]

# ---------------------------
# í•„ë“œë³„ ì½”í¼ìŠ¤ êµ¬ì„± (ì„¸ë¶€ ê°€ì¤‘ì¹˜ìš©)
# ---------------------------
titles = [r["title"] for r in filtered]
title_texts = [r["title"] + (" " + r["subtitle"] if r["subtitle"] else "") for r in filtered]
subject_texts = [" ".join(r["subjects"]) for r in filtered]
desc_texts = [r["desc"] for r in filtered]
author_texts = [r["creator"] for r in filtered]
publisher_texts = [r["publisher"] for r in filtered]
kdc_texts = [r["kdc"] for r in filtered]
years = [r["year"] for r in filtered]
pages = [r["pages"] for r in filtered]
raw_books = [r["raw"] for r in filtered]

# ë²¡í„°ë¼ì´ì €(í•„ë“œë³„)
vec_title = TfidfVectorizer()
vec_subj = TfidfVectorizer()
vec_desc = TfidfVectorizer()
vec_auth = TfidfVectorizer()
vec_pub  = TfidfVectorizer()
vec_kdc  = TfidfVectorizer()

X_title = vec_title.fit_transform(title_texts)
X_subj  = vec_subj.fit_transform(subject_texts)
X_desc  = vec_desc.fit_transform(desc_texts)
X_auth  = vec_auth.fit_transform(author_texts)
X_pub   = vec_pub.fit_transform(publisher_texts)
X_kdc   = vec_kdc.fit_transform(kdc_texts)

now_year = datetime.date.today().year
recency_vec = np.array([recency_weight(y, now_year) for y in years], dtype=float)

# ---------------------------
# ê°€ì¤‘ì¹˜ UI
# ---------------------------
st.sidebar.markdown("### âš–ï¸ ì„¸ë¶€ ê°€ì¤‘ì¹˜ (ì½˜í…ì¸ )")
w_title = st.sidebar.slider("ì œëª© ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.30, 0.05)
w_subj  = st.sidebar.slider("ì£¼ì œ(í‚¤ì›Œë“œ) ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.30, 0.05)
w_desc  = st.sidebar.slider("ì„¤ëª…(ìš”ì•½) ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.15, 0.05)
w_auth  = st.sidebar.slider("ì €ì ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.10, 0.05)
w_pub   = st.sidebar.slider("ì¶œíŒì‚¬ ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.05, 0.05)
w_kdc   = st.sidebar.slider("KDC ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.10, 0.05)

w_sum = w_title + w_subj + w_desc + w_auth + w_pub + w_kdc
if w_sum == 0:  # ì „ë¶€ 0ì´ë©´ ê¸°ë³¸ê°’ìœ¼ë¡œ
    w_title, w_subj, w_desc, w_auth, w_pub, w_kdc = 0.3, 0.3, 0.15, 0.1, 0.05, 0.1
    w_sum = 1.0

# ì •ê·œí™”
w_title, w_subj, w_desc, w_auth, w_pub, w_kdc = [w / w_sum for w in [w_title, w_subj, w_desc, w_auth, w_pub, w_kdc]]

st.sidebar.markdown("### â± ìµœê·¼ì„± ê°€ì¤‘ì¹˜")
w_recency = st.sidebar.slider("ìµœê·¼ 5ë…„ ê°€ì¤‘ì¹˜ ë¹„ìœ¨", 0.0, 0.8, 0.30, 0.05,
                              help="ìµœì¢… ì ìˆ˜ = (1-ë¹„ìœ¨)*ì½˜í…ì¸ ì ìˆ˜ + (ë¹„ìœ¨)*ìµœê·¼ì„±")

top_n = st.sidebar.slider("ì¶”ì²œ ê°œìˆ˜ (Top N)", 3, 15, 5)

def combine_content_score(sim_t, sim_s, sim_d, sim_a, sim_p, sim_k):
    return (w_title*sim_t + w_subj*sim_s + w_desc*sim_d + w_auth*sim_a + w_pub*sim_p + w_kdc*sim_k)

def final_score(content_sim, rec_vec):
    return (1 - w_recency) * content_sim + w_recency * rec_vec

# ---------------------------
# ë ˆì´ì•„ì›ƒ
# ---------------------------
col1, col2 = st.columns(2, vertical_alignment="top")

# ========== A) ì±… ì„ íƒí˜• ==========
with col1:
    st.subheader("ğŸ”– ì±… ì„ íƒí˜• ì¶”ì²œ")
    sel_title = st.selectbox("ì¶”ì²œ ê¸°ì¤€ì´ ë  ì±…ì„ ì„ íƒí•˜ì„¸ìš”", options=titles, index=0)
    if st.button("ì´ ì±…ê³¼ ë¹„ìŠ·í•œ ë„ì„œ ì¶”ì²œ", use_container_width=True):
        idx = titles.index(sel_title)
        # ê° í•„ë“œ ìœ ì‚¬ë„
        s_title = cosine_similarity(X_title[idx], X_title).flatten()
        s_subj  = cosine_similarity(X_subj[idx],  X_subj ).flatten()
        s_desc  = cosine_similarity(X_desc[idx],  X_desc ).flatten()
        s_auth  = cosine_similarity(X_auth[idx],  X_auth ).flatten()
        s_pub   = cosine_similarity(X_pub[idx],   X_pub  ).flatten()
        s_kdc   = cosine_similarity(X_kdc[idx],   X_kdc  ).flatten()

        content_sim = combine_content_score(s_title, s_subj, s_desc, s_auth, s_pub, s_kdc)
        final = final_score(content_sim, recency_vec)

        order = final.argsort()[::-1]
        recs = [i for i in order if i != idx][:top_n]

        st.write(f"**ê¸°ì¤€ ë„ì„œ:** {sel_title}")
        if not recs:
            st.info("ì¶”ì²œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            for i in recs:
                creator = to_text(raw_books[i].get("creator")) or "ì €ì ì •ë³´ ì—†ìŒ"
                kdc = to_text(raw_books[i].get("kdc")) or "N/A"
                y = years[i] or "N/A"
                p = pages[i] if pages[i] is not None else "N/A"
                st.markdown(
                    f"- **{titles[i]}** â€” {creator} (KDC: {kdc}, ì—°ë„: {y}, ìª½ìˆ˜: {p})  "
                    f"Â· ì½˜í…ì¸ ì ìˆ˜: {content_sim[i]:.3f} Â· ìµœì¢…ì ìˆ˜: {final[i]:.3f}"
                )

# ========== B) í‚¤ì›Œë“œ ê²€ìƒ‰í˜• ==========
with col2:
    st.subheader("ğŸ“ í‚¤ì›Œë“œ ê²€ìƒ‰í˜• ì¶”ì²œ")
    st.caption("ìƒìœ„ 10 í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ê±°ë‚˜, ììœ  í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    picked = st.multiselect("ìƒìœ„ í‚¤ì›Œë“œ", options=top_keywords, default=[])
    q = st.text_input("ììœ  í‚¤ì›Œë“œ (ì„ íƒ)", placeholder="ì˜ˆ: ë„ì„œê´€í•™, ì €ì‘ê¶Œë²•, ì—­ì‚¬")

    if st.button("í‚¤ì›Œë“œë¡œ ì¶”ì²œ", use_container_width=True):
        if not picked and not q.strip():
            st.warning("í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ê±°ë‚˜ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        else:
            # ì¿¼ë¦¬ í…ìŠ¤íŠ¸ êµ¬ì„± (í•„ë“œë³„ ë™ì¼ ì¿¼ë¦¬ ì‚¬ìš©)
            query = " ".join(picked + ([q.strip()] if q.strip() else []))

            q_title = vec_title.transform([query])
            q_subj  = vec_subj.transform([query])
            q_desc  = vec_desc.transform([query])
            q_auth  = vec_auth.transform([query])
            q_pub   = vec_pub.transform([query])
            q_kdc   = vec_kdc.transform([query])

            s_title = cosine_similarity(q_title, X_title).flatten()
            s_subj  = cosine_similarity(q_subj,  X_subj ).flatten()
            s_desc  = cosine_similarity(q_desc,  X_desc ).flatten()
            s_auth  = cosine_similarity(q_auth,  X_auth ).flatten()
            s_pub   = cosine_similarity(q_pub,   X_pub  ).flatten()
            s_kdc   = cosine_similarity(q_kdc,   X_kdc  ).flatten()

            content_sim = combine_content_score(s_title, s_subj, s_desc, s_auth, s_pub, s_kdc)
            final = final_score(content_sim, recency_vec)

            order = final.argsort()[::-1][:top_n]
            st.write(f"**ì…ë ¥/ì„ íƒ í‚¤ì›Œë“œ:** {query}")
            for i in order:
                creator = to_text(raw_books[i].get("creator")) or "ì €ì ì •ë³´ ì—†ìŒ"
                kdc = to_text(raw_books[i].get("kdc")) or "N/A"
                y = years[i] or "N/A"
                p = pages[i] if pages[i] is not None else "N/A"
                st.markdown(
                    f"- **{titles[i]}** â€” {creator} (KDC: {kdc}, ì—°ë„: {y}, ìª½ìˆ˜: {p})  "
                    f"Â· ì½˜í…ì¸ ì ìˆ˜: {content_sim[i]:.3f} Â· ìµœì¢…ì ìˆ˜: {final[i]:.3f}"
                )
